{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nishzsche/nishzsche.github.io/blob/gh-pages/PoorOrpo_Finetune_and_evaluate_an_LLM_on_a_budget.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí∏ PoorOrpo: Finetuning LLMs on a budget\n",
        "\n",
        "This notebook is an condensed end-to-end LLM finetuning guide. It finetunes, evaluates, and infers an LLM. All from this little notebook. üêÅ\n"
      ],
      "metadata": {
        "id": "Pa9y4NtnLVKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "# @title ## ‚Ñπ 1. Setup your finetuning project\n",
        "\n",
        "# @markdown Add your personal project information Colab secrets to deploy models, monitoring, and results.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### üßë Project Configuration\n",
        "\n",
        "## @markdown The huggingface hub repo id to push the final quantized model:\n",
        "\n",
        "TUNED_MODEL_ID = \"burtenshaw/Qwen1.5-0.5B-dpo-mix-7k\" # @param {type:\"string\"}\n",
        "TUNED_MODEL_NAME = TUNED_MODEL_ID.split(\"/\")[1]\n",
        "## @markdown The weights & biases project to log results to during training:\n",
        "WANDB_PROJECT=\"skintstack-orpo\" # @param {type:\"string\"}\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown ### üîê Credentials\n",
        "\n",
        "HF_USERNAME=\"burtenshaw\" # @param {type:\"string\"}\n",
        "HF_TOKEN=\"HF_TOKEN\" # @param {type:\"string\"}\n",
        "WANDB_TOKEN= \"WANDB_TOKEN\" # @param {type:\"string\"}\n",
        "# RUNPOD_TOKEN = \"RUNPOD_TOKEN\" # @param {type:\"string\"}\n",
        "# GITHUB_TOKEN = \"GITHUB_TOKEN\" # @param {type:\"string\"}\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "WANDB_TOKEN = userdata.get('WANDB_TOKEN')\n",
        "# RUNPOD_TOKEN = userdata.get(\"RUNPOD_TOKEN\")\n",
        "\n",
        "\n",
        "# @markdown ---"
      ],
      "metadata": {
        "id": "XfL5P7F1xPbN",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## üöÜ 2. Finetune a base model with ORPO\n",
        "\n",
        "# @markdown We will train the model using [ORPO](https://huggingface.co/papers/2403.07691), because it outperforms `SFT, SFT+DPO` on `PHI-2, Llama 2, and Mistral`.\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown ### ü§ó Training Parameters\n",
        "\n",
        "config = \"\"\"compute_environment: LOCAL_MACHINE\n",
        "debug: false\n",
        "distributed_type: FSDP\n",
        "downcast_bf16: 'no'\n",
        "fsdp_config:\n",
        "  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP\n",
        "  fsdp_backward_prefetch: BACKWARD_PRE\n",
        "  fsdp_cpu_ram_efficient_loading: true\n",
        "  fsdp_forward_prefetch: true\n",
        "  fsdp_offload_params: false\n",
        "  fsdp_sharding_strategy: FULL_SHARD\n",
        "  fsdp_state_dict_type: FULL_STATE_DICT\n",
        "  fsdp_sync_module_states: true\n",
        "  fsdp_use_orig_params: true\n",
        "machine_rank: 0\n",
        "main_training_function: main\n",
        "mixed_precision: bf16\n",
        "num_machines: 1\n",
        "num_processes: 1\n",
        "rdzv_backend: static\n",
        "same_network: true\n",
        "tpu_env: []\n",
        "tpu_use_cluster: false\n",
        "tpu_use_sudo: false\n",
        "use_cpu: false\"\"\"\n",
        "\n",
        "with open(\"orpo/train.yml\", \"w\") as f:\n",
        "    f.write(config)\n",
        "\n",
        "BASE_MODEL_ID = \"Qwen/Qwen1.5-0.5B\" # @param {type:\"string\"}\n",
        "DATASET = \"argilla/dpo-mix-7k\" # @param {type:\"string\"}\n",
        "EPOCH = 1 # @param {type:\"integer\"}\n",
        "LEARNING_RATE = 5e-6 # @param {type:\"number\"}\n",
        "BATCH_SIZE = 2 # @param [1,2,4,8,16]\n",
        "TRUST_REMOTE_CODE = True\n",
        "\n",
        "# @markdown ---\n",
        "!pip install -qqq torch>=1.10 datasets accelerate wandb transformers bitsandbytes sentencepiece --progress-bar off\n",
        "!wandb login $WANDB_TOKEN\n",
        "!wandb init -p $WANDB_PROJECT\n",
        "!git clone https://github.com/xfactlab/orpo.git\n",
        "\n",
        "!cd orpo && accelerate launch --config_file train.yml main.py \\\n",
        "    --lr {LEARNING_RATE} \\\n",
        "    --warmup_steps 100 \\\n",
        "    --model_name {BASE_MODEL_ID} \\\n",
        "    --data_name {DATASET} \\\n",
        "    --num_train_epochs {EPOCH} \\\n",
        "    --prompt_max_length 128 \\\n",
        "    --response_max_length 2048 \\\n",
        "    --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "    --per_device_eval_batch_size {BATCH_SIZE} \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --num_proc 1\n",
        "\n",
        "OUTPUT = \"checkpoints/\"+ DATASET.split(\"/\")[-1]\n",
        "!echo \"++++++ Publishing to the Hub ++++++\"\n",
        "!huggingface-cli login --token {HF_TOKEN}\n",
        "!huggingface-cli upload {TUNED_MODEL_NAME} orpo/{OUTPUT}\n"
      ],
      "metadata": {
        "id": "HXX83z9-kJ0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # üßê 3. Evaluate the finetuned model with (with [lm-evaluation-harness by EleutherAI](https://github.com/EleutherAI/lm-evaluation-harness))\n",
        "\n",
        "!git clone https://github.com/EleutherAI/lm-evaluation-harness\n",
        "!cd lm-evaluation-harness && pip install -qqq -e .[ifeval] --progress-bar off\n",
        "!pip install accelerate\n",
        "!pip install -q huggingface_hub\n",
        "\n",
        "import json\n",
        "from huggingface_hub import create_repo, HfApi, ModelCard, EvalResult, ModelCardData\n",
        "\n",
        "# @markdown Select the benchmark you want to use.\n",
        "benchmark=\"ifeval\" #@param [\"eq-bench\", \"ifeval\", \"hellaswag\"]\n",
        "publish_to_hub = True #@param\n",
        "!echo \"================== $(echo $benchmark | tr '[:lower:]' '[:upper:]') [1/6] ==================\"\n",
        "!accelerate launch -m lm_eval \\\n",
        "    --model hf \\\n",
        "    --model_args pretrained={TUNED_MODEL_ID},dtype=auto,trust_remote_code=true \\\n",
        "    --tasks {benchmark} \\\n",
        "    --num_fewshot 0 \\\n",
        "    --batch_size auto:4 \\\n",
        "    --output_path ./{TUNED_MODEL_ID}-{benchmark}.json \\\n",
        "    --verbosity \"critical\"\n",
        "\n",
        "benchmark = \"eq-bench\"\n",
        "\n",
        "with open(f\"{TUNED_MODEL_ID}-{benchmark}.json\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(results)\n",
        "if publish_to_hub:\n",
        "    eval_results = []\n",
        "    for benchmark, config in results[\"configs\"].items():\n",
        "        dataset_name = config[\"dataset_path\"]\n",
        "        for metric in config[\"metric_list\"]:\n",
        "            metric_name = metric.pop(\"metric\")\n",
        "            score = results[\"results\"][benchmark][f\"{metric_name},none\"]\n",
        "            eval_result = EvalResult(\n",
        "                task_name=benchmark,\n",
        "                task_type=\"question-answering\",\n",
        "                dataset_name=dataset_name,\n",
        "                dataset_type=\"benchmark\",\n",
        "                metric_type=\"accuracy\",\n",
        "                metric_value=score,\n",
        "                metric_name=metric_name,\n",
        "                # metric_args=metric\n",
        "            )\n",
        "            eval_results.append(eval_result)\n",
        "\n",
        "\n",
        "\n",
        "    # Create model card\n",
        "    card_data = ModelCardData(\n",
        "        language='en',\n",
        "        license='mit',\n",
        "        model_name=TUNED_MODEL_NAME,\n",
        "        eval_results = eval_results\n",
        "    )\n",
        "\n",
        "    card = ModelCard.from_template(card_data)\n",
        "    card.save(f'{TUNED_MODEL_NAME}/README.md')\n",
        "    card.push_to_hub(TUNED_MODEL_ID, token=HF_TOKEN)"
      ],
      "metadata": {
        "id": "NB_XM2yyPBbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üí° 4. Use the model with!\n",
        "\n",
        "Use a HF serverless endpoint to infer with your new model! You could also deploy your model to ChatUI space from [here](https://huggingface.co/new-space?template=huggingchat/chat-ui-template)."
      ],
      "metadata": {
        "id": "8gwk19uh7Q8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = f\"https://api-inference.huggingface.co/models/{TUNED_MODEL_ID}\"\n",
        "headers = {\"Authorization\": f\"Bearer {HF_TOKEN}\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": \"Can you please let us know more details about your \",\n",
        "})\n",
        "\n",
        "print(output[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtU1V7957LMS",
        "outputId": "e3342fbe-3391-4c62-9e30-3a0a85eb5d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Can you please let us know more details about your 2019-2020 schedule? We are looking forward to hearing from you.\n",
            "We are currently in the process of updating our schedule. We will be posting updates as soon as we have them.\n"
          ]
        }
      ]
    }
  ]
}